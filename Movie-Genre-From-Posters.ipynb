{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################\n",
    "\n",
    "def read_image_data(file_path, bool_images):\n",
    "    images_arr = []\n",
    "    id_genres_list_ = []\n",
    "    '''lists all the directories in file_path'''\n",
    "    inner_dirs = [path for path in glob.glob(file_path)]\n",
    "    \n",
    "    if bool_images:\n",
    "        for path in inner_dirs:\n",
    "\n",
    "            images = []\n",
    "            images_names_list = (os.listdir(path))\n",
    "            images_arr = read_images(path, images_names_list, images_arr)\n",
    "\n",
    "        return (images_arr)\n",
    "    \n",
    "    else:\n",
    "        for path in inner_dirs:\n",
    "            \n",
    "            id_genres_list_ = id_genres_list_ + func_to_read_id_genre_data(path)\n",
    "            \n",
    "        \n",
    "        return id_genres_list_\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "###############################################################\n",
    "\n",
    "def read_images(path, images_names, images_arr):\n",
    "    for image_name in images_names:\n",
    "        resized_image = cv2.resize(cv2.imread(path + \"/\" + image_name), (300, 400))\n",
    "        resized_image = resized_image / 255\n",
    "        img_name_ = image_name[0 : image_name.index('.jpg')]\n",
    "        \n",
    "        '''adding the image and its id as an ordered pair to the images_arr'''\n",
    "        images_arr.append((img_name_, resized_image))\n",
    "\n",
    "    return images_arr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def read_image_genre_id(path, encoding_):\n",
    "    #genres_set = {}\n",
    "    id_genres_list = []\n",
    "    id_val = ''\n",
    "    genres = []\n",
    "    id_flag = False\n",
    "    genre_flag = False\n",
    "  \n",
    "    with open(path, encoding = encoding_) as file:\n",
    "        for line in file:\n",
    "            \n",
    "            arr = line.split(':')\n",
    "            if len(arr) > 1:\n",
    "                '''removes non-alphanumerics from the string'''\n",
    "                id_ = ''.join(e for e in arr[0] if e.isalnum())\n",
    "                if (id_) == 'imdbID':\n",
    "                    id_flag = True\n",
    "                    id_val = ''.join(e for e in arr[1] if e.isalnum())\n",
    "                    \n",
    "                if id_ == 'Genre':\n",
    "                    genre_flag = True\n",
    "                    genres_ = arr[1][0 : -2].split(',')\n",
    "                    for genre in genres_:\n",
    "                        word = ''.join(e for e in genre if e.isalnum())\n",
    "                        genres.append(word)\n",
    "\n",
    "                if id_flag and genre_flag:\n",
    "\n",
    "                    id_genre_pair = (id_val, genres)\n",
    "                    id_genres_list.append(id_genre_pair)\n",
    "                    \n",
    "                    \n",
    "                    id_flag = False\n",
    "                    genre_flag = False\n",
    "                    id_val = ''\n",
    "                    genres = []\n",
    "\n",
    "        \n",
    "        return id_genres_list\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def func_to_read_id_genre_data(path):\n",
    "    \n",
    "    try:\n",
    "        return read_image_genre_id(path, 'utf-16-le')\n",
    "    \n",
    "    except UnicodeDecodeError:\n",
    "        return read_image_genre_id(path, 'utf-8')\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "###############################################################\n",
    "\n",
    "def convert_id_genre_list_to_df(imageid_genre_list):\n",
    "    id_genre_dicts_list = []\n",
    "    for id_genre in image_id_genre_list:\n",
    "        id_genre_dict = {}\n",
    "        id_genre_dict['movie_Id'] = id_genre[0]\n",
    "        for genre in id_genre[1]:\n",
    "            id_genre_dict[genre] = 1\n",
    "        \n",
    "        id_genre_dicts_list.append(id_genre_dict)\n",
    "    \n",
    "    return pd.DataFrame(id_genre_dicts_list)\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def convert_image_list_to_df(image_list):\n",
    "    return pd.DataFrame(image_list, columns = ['movie_Id', 'pixel_data'])\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def fill_NaNs_with_Zeros(id_genre_df):\n",
    "    return id_genre_df.fillna(0)\n",
    "\n",
    "\n",
    "###############################################################\n",
    "\n",
    "def merge_image_data_id_genre_Dfs(image_df, id_genre_df):\n",
    "    return pd.merge(image_df, id_genre_df, on = 'movie_Id')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------*********************************--------------------------####\n",
    "\n",
    "\n",
    "def get_features_and_labels(image_id_genre_data_df):\n",
    "    feature_columns_ = []\n",
    "    df_columns = image_id_genre_data_df.columns\n",
    "    label_columns = [column for column in df_columns if column not in ['movie_Id', 'pixel_data']]\n",
    "    \n",
    "    pixel_vals = image_id_genre_data_df['pixel_data'].values\n",
    "    for val in pixel_vals:\n",
    "        feature_columns_.append(val)\n",
    "        \n",
    "    #feature_data = image_id_genre_data_df.loc[:, 'pixel_data']\n",
    "    label_data   = image_id_genre_data_df.loc[:, label_columns]\n",
    "    \n",
    "    return np.array(feature_columns_), label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = read_image_data(\"/Users/vijay/Downloads/Datasets/movie_poster_data/*\", True)\n",
    "image_id_genre_list = read_image_data(\"/Users/vijay/Downloads/Datasets/ground_truth/*\", False)\n",
    "\n",
    "id_genre_df = fill_NaNs_with_Zeros(convert_id_genre_list_to_df(image_id_genre_list))\n",
    "image_df = convert_image_list_to_df(image_list)\n",
    "\n",
    "del(image_list)\n",
    "del(image_id_genre_list)\n",
    "\n",
    "image_id_genre_pixel_data_df = merge_image_data_id_genre_Dfs(image_df, id_genre_df)\n",
    "\n",
    "del(image_df)\n",
    "#del(image_id_genre_list)\n",
    "\n",
    "X_data, label_data = get_features_and_labels(image_id_genre_pixel_data_df)\n",
    "\n",
    "del(image_id_genre_pixel_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_the_train_data(train_X, train_Y):\n",
    "    train_x, train_y, test_x, test_y = train_test_split(train_X, train_Y, test_size = 0.2, \n",
    "                                                        random_state = 42)\n",
    "    return [[train_x, test_x], [train_y, test_y]]\n",
    "\n",
    "\n",
    "\n",
    "def get_train_and_label_data(train_data, test_data):\n",
    "\n",
    "    no_of_rows = np.shape(train_data)[0]\n",
    "#     no_of_train_samples = int(math.modf(no_of_rows * 0.8)[1])\n",
    "    no_of_train_samples = 600\n",
    "    train_X = train_data[0 : no_of_train_samples]\n",
    "    train_Y = test_data[0 : no_of_train_samples]\n",
    "    \n",
    "    #test_X = train_data.loc[35000: ,]\n",
    "    test_X = train_data[no_of_train_samples : ]\n",
    "    test_Y = test_data[no_of_train_samples : ]\n",
    "    \n",
    "    return [[train_X, train_Y], [test_X, test_Y]]\n",
    "\n",
    "\n",
    "\n",
    "train_test_data  = get_train_and_label_data(X_data, label_data.values)\n",
    "train_X, train_Y = train_test_data[0]\n",
    "test_X, test_Y   = train_test_data[1]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TENSORFLOW MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_data_shape = np.shape(X_data)\n",
    "batch_size = 100\n",
    "height = x_data_shape[1]\n",
    "width  = x_data_shape[2]\n",
    "no_of_channels = x_data_shape[3]\n",
    "no_of_labels = np.shape(label_data)[1]\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 400 * 300 * 3])\n",
    "x_rs = tf.reshape(x, [-1, 400, 300, 3])\n",
    "\n",
    "y = tf.placeholder(tf.float32, [None, no_of_labels])\n",
    "y_rs = tf.reshape(y, [-1, no_of_labels])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_batch_size(no_of_rows):\n",
    "    batch_size = 1\n",
    "    for i in range(2, 51):\n",
    "        if no_of_rows % i ==0:\n",
    "            if i > batch_size:\n",
    "                batch_size = i\n",
    "            \n",
    "    return batch_size\n",
    "\n",
    "\n",
    "\n",
    "def get_total_no_of_neurons(shape_of_conv_out):\n",
    "    neurons = 1\n",
    "    for num in shape_of_conv_out:\n",
    "        if num is not None:\n",
    "            neurons = neurons * num\n",
    "    return neurons\n",
    "\n",
    "\n",
    "\n",
    "def get_weights_and_bias_for_conv_and_pooling():\n",
    "    w_1 = tf.Variable(tf.truncated_normal([5, 5, no_of_channels, 16], stddev = 0.2, seed = 23, dtype = tf.float32), name = 'w_1')\n",
    "    b_1 = tf.Variable(tf.constant(0.1, shape = [16], dtype = tf.float32), name = 'b_1')\n",
    "    \n",
    "    w_2 = tf.Variable(tf.truncated_normal([5, 5, 16, 32], stddev = 0.2, seed = 23, dtype = tf.float32), name = 'w_2')\n",
    "    b_2 = tf.Variable(tf.constant(0.1, shape = [32], dtype = tf.float32), name = 'b_2')\n",
    "    \n",
    "    w_3 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev = 0.2, seed = 23, dtype = tf.float32), name = 'w_3')\n",
    "    b_3 = tf.Variable(tf.constant(0.1, shape = [64], dtype = tf.float32), name = 'b_3')\n",
    "    \n",
    "    w_4 = tf.Variable(tf.truncated_normal([5, 5, 64, 64], stddev = 0.2, seed = 23, dtype = tf.float32), name = 'w_3')\n",
    "    b_4 = tf.Variable(tf.constant(0.1, shape = [64], dtype = tf.float32), name = 'b_3')\n",
    "    \n",
    "    return [[w_1, b_1], [w_2, b_2], [w_3, b_3], [w_4, b_4]]\n",
    "\n",
    "\n",
    "\n",
    "def get_weights_and_bias_for_fc_layers(conv_result_shape):\n",
    "    \n",
    "    total_no_of_neurons = get_total_no_of_neurons(conv_result_shape)\n",
    "    \n",
    "    fc_w_1 = tf.Variable(tf.truncated_normal([total_no_of_neurons, 128], stddev = 0.2, seed = 23, dtype = tf.float32), name = 'fc_w_1')\n",
    "    fc_b_1 = tf.Variable(tf.constant(0.1, shape = [128], dtype = tf.float32), name = 'fc_b_1')\n",
    "    \n",
    "    fc_w_2 = tf.Variable(tf.truncated_normal([128, 64], stddev = 0.2, seed = 23, dtype = tf.float32), name = 'fc_w_2')\n",
    "    fc_b_2 = tf.Variable(tf.constant(0.1, shape = [64], dtype = tf.float32), name = 'fc_b_1')\n",
    "    \n",
    "    fc_w_3 = tf.Variable(tf.truncated_normal([64, no_of_labels], stddev = 0.2, seed = 23, dtype = tf.float32), name = 'fc_w_3')\n",
    "    fc_b_3 = tf.Variable(tf.constant(0.1, shape = [no_of_labels], dtype = tf.float32), name = 'fc_b_3')\n",
    "    \n",
    "    return [[fc_w_1, fc_b_1], [fc_w_2, fc_b_2], [fc_w_3, fc_b_3]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def apply_convolution_on_the_data(x, w, b):\n",
    "    \n",
    "    convoluted_image = tf.nn.conv2d(x, w, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "    CI_with_bias     = tf.nn.bias_add(convoluted_image, b)\n",
    "    activated_CI     = tf.nn.relu(CI_with_bias)\n",
    "    return activated_CI\n",
    "\n",
    "\n",
    "\n",
    "def apply_max_pooling_on_convolution(convolution, k = 2):\n",
    "    return tf.nn.max_pool(convolution, ksize = [1, k, k, 1], strides = [1, k, k, 1], padding = 'SAME')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def perform_convolution_and_pooling_on_data():\n",
    "    \n",
    "    weights_biases = get_weights_and_bias_for_conv_and_pooling()\n",
    "    w_1, b_1       = weights_biases[0]\n",
    "    w_2, b_2       = weights_biases[1]\n",
    "    w_3, b_3       = weights_biases[2]\n",
    "    w_4, b_4       = weights_biases[3]\n",
    "    \n",
    "    \n",
    "    conv_1 = apply_convolution_on_the_data(x_rs, w_1, b_1)\n",
    "    pool_1 = apply_max_pooling_on_convolution(conv_1)\n",
    "    norm_1 = tf.nn.lrn(pool_1, depth_radius = 4, bias = 1.0, alpha = 0.002 / 9.0, beta = 0.75)\n",
    "    \n",
    "    \n",
    "    conv_2 = apply_convolution_on_the_data(norm_1, w_2, b_2)\n",
    "    pool_2 = apply_max_pooling_on_convolution(conv_2)\n",
    "    norm_2 = tf.nn.lrn(pool_2, depth_radius = 4, bias = 1.0, alpha = 0.002 / 9.0, beta = 0.75)\n",
    "    \n",
    "    conv_3 = apply_convolution_on_the_data(norm_2, w_3, b_3)\n",
    "    pool_3 = apply_max_pooling_on_convolution(conv_3)\n",
    "    norm_3 = tf.nn.lrn(pool_3, depth_radius = 4, bias = 1.0, alpha = 0.002 / 9.0, beta = 0.75)\n",
    "    \n",
    "    conv_4 = apply_convolution_on_the_data(norm_3, w_4, b_4)\n",
    "    pool_4 = apply_max_pooling_on_convolution(conv_3)\n",
    "    norm_4 = tf.nn.lrn(pool_4, depth_radius = 4, bias = 1.0, alpha = 0.002 / 9.0, beta = 0.75)\n",
    "\n",
    "    return norm_4\n",
    "\n",
    "\n",
    "def form_fc_layers_and_perfom_activations(conv_result):\n",
    "    \n",
    "    shape_of_conv_result   = conv_result.get_shape().as_list()\n",
    "    fc_weights_biases = get_weights_and_bias_for_fc_layers(shape_of_conv_result)\n",
    "    \n",
    "    fc_w_1, fc_b_1    = fc_weights_biases[0]\n",
    "    fc_w_2, fc_b_2    = fc_weights_biases[1]\n",
    "    fc_w_3, fc_b_3    = fc_weights_biases[2]\n",
    "    \n",
    "    \n",
    "    conv_res_reshaped = tf.reshape(conv_result, [-1, fc_w_1.get_shape().as_list()[0]])\n",
    "    fc_layer_1        = tf.add(tf.matmul(conv_res_reshaped, fc_w_1), fc_b_1)\n",
    "    fc_relu_1         = tf.nn.relu(fc_layer_1)\n",
    "\n",
    "    \n",
    "    fc_layer_2        = tf.add(tf.matmul(fc_relu_1, fc_w_2), fc_b_2)\n",
    "    fc_relu_2         = tf.nn.relu(fc_layer_2)\n",
    "    fc_2_dropout      = tf.nn.dropout(fc_relu_2, keep_prob)\n",
    "    \n",
    "    fc_layer_out      = tf.add(tf.matmul(fc_2_dropout, fc_w_3), fc_b_3)\n",
    "    \n",
    "    return fc_layer_out\n",
    "\n",
    "\n",
    "def CNN_movie_image():\n",
    "    \n",
    "    conv_result   = perform_convolution_and_pooling_on_data()\n",
    "    fc_layer_out  = form_fc_layers_and_perfom_activations(conv_result)\n",
    "    \n",
    "    return fc_layer_out\n",
    "\n",
    "\n",
    "def perform_optimization():\n",
    "    fc_layer_res         = CNN_movie_image()\n",
    "    cost                 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y_rs, logits = fc_layer_res))\n",
    "    optimizer            = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)\n",
    "    correctly_predicted  = tf.equal(tf.argmax(fc_layer_res, 1), tf.argmax(y_rs, 1))\n",
    "    accuracy             = tf.reduce_mean(tf.cast(correctly_predicted, tf.float32))\n",
    "    \n",
    "    return fc_layer_res, optimizer, accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_CNN_for_movie_Genre():\n",
    "    with tf.Session() as session:\n",
    "        fc_layer_result, optimizer, accuracy  = perform_optimization()\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        saver      = tf.train.Saver(tf.trainable_variables())\n",
    "        #batch_size = get_batch_size(np.shape(train_X)[0])\n",
    "        #print('batch size ' + str(batch_size))\n",
    "        for epoch in range(0, 3):\n",
    "            for i in range(0, len(train_X), batch_size):\n",
    "                \n",
    "                train_x         = train_X[i : i + batch_size]\n",
    "                train_y         = train_Y[i : i + batch_size]\n",
    "                \n",
    "                _, accuracy_val = session.run([optimizer, accuracy], feed_dict = {x_rs : train_x, \n",
    "                                                                                  y_rs : train_y, \n",
    "                                                                                  keep_prob: 0.5})\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                print(epoch, accuracy_val)\n",
    "\n",
    "    \n",
    "        save_path = saver.save(session, \"./mov_gen_img_model\")\n",
    "        \n",
    "        return fc_layer_result\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_out = run_CNN_for_movie_Genre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    meta_graph = tf.train.import_meta_graph('./mov_gen_img_model.meta')\n",
    "    meta_graph.restore(session, tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    prediction = session.run(fc_layer_out, feed_dict = {x_rs : test_X})\n",
    "    print((session.run(tf.nn.sigmoid(prediction))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
